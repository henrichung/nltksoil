---
title: "2. Results"
author: "Henri Chung"
date: "7/21/2020"
output: html_document
---

```{r setup, eval=FALSE}
list.of.packages <- c("tidyverse","tidymodels",  "reshape2", "furrr", "future", "doParallel", 
                      "text2vec", "stringi")#,
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages, repos = "https://cloud.r-project.org")
invisible(capture.output(lapply(list.of.packages, library, character.only = TRUE, warn.conflicts = FALSE, quietly = T)))
rm(list = ls())
source("code/nltksoil_functions.R")
```

### Data Processing

For each iteration of our model, we save an individual RDS object to store the results. In order to analyze the results for comparison, we have to combine these individual results table into a cumulative dataframe for plotting. It is important to make sure the data is in tidy format. 

```{r, eval = FALSE}
outputFolder <- "../../../../work/idoerg/hchung/outputs/soil"
##random forest and 50 dimensions
results_files <- list.files(outputFolder)
results_filenames <- paste(outputFolder, results_files, sep = "/")
results_list_tune <- list(); 
results_list_test <- list(); 
results_list_models <- list()
results_list_best <- list()
for(i in 1:length(results_files)){
	temp <- readRDS(results_filenames[i])
	results_list_tune[[i]] <- temp$tune
	results_list_test[[i]] <- temp$test
  results_list_models[[i]] <- temp$model
  results_list_best[[i]] <- temp$best
	if(i %% 5 == 0){message(i)}
}
names(results_list_tune) <- results_files; names(results_list_test) <- results_files
names(results_list_best) <- results_files; names(results_list_models) <- results_files
results_tune_raw <- bind_rows(results_list_tune, .id = "filename")
results_test_raw <- bind_rows(results_list_test, .id = "filename")
results_best_raw <- bind_rows(results_list_best, .id = "filename")

saveRDS(results_list_models, "results_models.RDS")

```

### Bind and process

We bind our data tables into one cumulative dataframe. We can then calculate model performance metrics for multiclass classifiers on our predictions. There are 3 ways to measure multiclass performance.

+ **macro averaging** - reduce multiclass predictions to multiple sets of binary (all vs one) predictions, and calcualtes the corresponding metric for each of the binary cases. Take the average of all class metrics to evaluate performance.  Each class gets equal weight (bad for imbalanced dataset)

+ **weighted macro average** - same as previous except metrics are weighed by existing proportion in data.

+ **micro average** - treats the entire dataset as an aggregate result, and calculates 1 metric rather than k metrics averaged together. For example; 
    + Precision_macro = [tp_1 / (tp_1 + fp_1)] + [tp_2 / (tp_2 + fp_2)] / 2
    + Precision_micro [tp_1 + tp_2 / (tp_1 + tp_2) + (fp_1 + fp_2)]
 
For completeness, we use all 3 methods of averaging across all metrics. We include metrics;

+ **accuracy** - the proportion of the data that are predicted correctly.

+ **bal_accuracy** - Average of sensitivity and specificity

+ **f_meas** - Harmonic mean of the precision and recall

+ **kap** - accuracy relative to random chance/the proportion of classes in dataset

+ **npv** - Proportion of negative identification that were identified correctly

+ **ppv/precision** - proportion of positive identifications that were identified correctly

+ **recall/sensitivity** - proportion of actual positives that were identified correctly

+ **specificity** - the proportion of negatives that are correctly identified as negatives.

+ **pr_auc** - area underneath the precision recall curve.

+ **roc_auc** - area underneath the reciever operating characteristic curve.

```{r, eval = FALSE}
error_multi_metric <- function (x) {
  multi_metric <- metric_set(accuracy, bal_accuracy, f_meas, kap, npv, ppv, precision, recall, sens, pr_auc, roc_auc)
  levels = paste(as.character(levels(x$response)), collapse = "|")
  res <- multi_metric(x, truth = "response", estimate = ".pred_class", ... = matches(levels), na_rm = TRUE, estimator = "macro_weighted")
  return(res)
}

error_multi_metric2 <- function (x) {
  multi_metric <- metric_set(accuracy, bal_accuracy, f_meas, kap, npv, ppv, precision, recall, sens)
  levels = paste(as.character(levels(x$response)), collapse = "|")
  res <- multi_metric(x, truth = "response", estimate = "estimate", ... = matches(levels), na_rm = TRUE, estimator = "macro_weighted")
  return(res)
}
plan(multiprocess)
results_tune <- results_tune_raw %>%
	mutate(filename = gsub("envo_biome_2", "envobiome2", filename)) %>%
	separate(filename, c(".taxa", ".response", ".model", ".no_components", ".method")) %>%
  mutate(.method = factor(.method, levels = c("base", "abundance", "transformpca", "transformword", "pca", "word"))) %>%
  mutate(.no_components = factor(.no_components, levels = c(50, 100, 150, 200, 250, 300))) %>%
  mutate(.taxa = factor(.taxa, levels = c("species", "genus","family", "order", "class", "phylum"))) %>%
  mutate(.metrics = future_map(.predictions, error_multi_metric)) %>%
  mutate(.custom = future_map(.predictions, custom_evaluate)) %>%
  mutate(.pr_curve = future_map(.predictions, custom_pr_curve)) %>%
  mutate(.roc_curve = future_map(.predictions, custom_roc_curve)) %>%
  mutate(.null = future_map(.null, ~mutate(., estimate = as.factor(estimate)))) %>%
  mutate(.null = future_map(.null, ~refactor(., "response", "estimate"))) %>%
  mutate(.nullmetrics = future_map(.null, error_multi_metric2))

saveRDS(results_tune, "results_tune.RDS")

results_best <- results_best_raw %>%
  mutate(filename = gsub("envo_biome_2", "envobiome2", filename)) %>%
  separate(filename, c(".taxa", ".response", ".model", ".no_components", ".method")) %>%
  mutate(.method = factor(.method, levels = c("base", "abundance", "transformpca", "transformword", "pca", "word"))) %>%
  mutate(.no_components = factor(.no_components, levels = c(50, 100, 150, 200, 250, 300))) %>%
  mutate(.taxa = factor(.taxa, levels = c("species", "genus","family", "order", "class", "phylum"))) 
saveRDS(results_best, "results_best.RDS")

results_test <- results_test_raw %>%
	mutate(filename = gsub("envo_biome_2", "envobiome2", filename)) %>%
  separate(filename, c(".taxa", ".response", ".model", ".no_components", ".method")) %>%
  mutate(.method = factor(.method, levels = c("base", "abundance", "transformpca", "transformword", "pca", "word"))) %>%
  mutate(.no_components = factor(.no_components, levels = c(50, 100, 150, 200, 250, 300))) %>%
  mutate(.taxa = factor(.taxa, levels = c("species", "genus","family", "order", "class", "phylum"))) %>%
  mutate(.predictions = future_map(.predictions, ~mutate(., response = droplevels(response), .pred_class = droplevels(.pred_class)))) %>%
  mutate(.predictions = future_map(.predictions, ~refactor(., "response", ".pred_class"))) %>%
  mutate(.metrics = future_map(.predictions, error_multi_metric)) %>%
	mutate(.custom =future_map(.predictions, custom_evaluate)) %>%
  mutate(.pr_curve = future_map(.predictions, custom_pr_curve)) %>%
  mutate(.roc_curve = future_map(.predictions, custom_roc_curve)) %>%
  mutate(.null = future_map(.null, ~mutate(., estimate = as.factor(estimate)))) %>%
  mutate(.null = future_map(.null, ~refactor(., "response", "estimate"))) %>%
  mutate(.nullmetrics = future_map(.null, error_multi_metric2))
saveRDS(results_test, "results_test.RDS")

```
