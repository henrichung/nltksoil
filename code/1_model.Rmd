---
title: "nltk soil"
author: "Henri Chung"
date: "7/4/2020"
output: html_document
---

```{r setup, include=FALSE}
## Section A: Set up libraries and load data
##################################################
list.of.packages <- c("tidyverse","tidymodels",  "reshape2", "furrr", "future", "doParallel", 
                      "text2vec", "stringi")#, "Rcpp", "profvis","Matrix", "themis", "stringi", "data.table", "tictoc")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages, repos = "https://cloud.r-project.org")
invisible(capture.output(lapply(list.of.packages, library, character.only = TRUE, warn.conflicts = FALSE, quietly = T)))

rm(list = ls()) #clear environment
args <- commandArgs(trailingOnly = TRUE)
source("../code/nltksoil_functions.R") #Load functions
```

### Background

The purpose of this R notebook is to evaluate the performance of word embedding methods in supervised machine learning to predict host traits from microbiome data. The microbiome is defined as the collection of all living microbes in or around a host. This data, analogous to a list of which bacterial species are present at what location, has been shown to be strongly indicative of host traits. Previous studies have used microbiome data to predict for the presence and risk of certain diseases. Other traits, such as environmental characteristics, have been found to be strongly associated with microbiome data, and can be predicted from the microbiome with a reasonable degree of accuracy.

Due to the development of modern high-throughput methods, microbiome data is abundant. A common method of recording sequencing data is to use Operational Taxonomic Units (OTUs), which clusters highly similar sequences (ex: ~97%) together to represent a single taxa or OTU. We then record the count of unique OTUs in a sample. For a single sample, this data is stored in a vector of equal length to the number of unique taxa. An OTU table is multiple of these vectors bound together into a matrix, where the number of rows is equal to the number of samples and the number of columns is equal to the number of unique taxa summed across all samples. The respective cell values then record the count of each column taxa in each row sample. The dimensionality of these tables can grow very quickly, as the number of unique taxa increases across large group of diverse samples. OTU tables are also sparse, the cumulative number of possible unique taxa can greatly exceed the number of taxa in a single sample several-fold, leading to many zero elements.

The sparsity and high dimensionality of OTU tables make microbiome data difficult to work with when building predictive models. However, there are methods that are suited to this type of data. Machine learning methods such as random forest models or neural networks are commonly used to build models with high dimension data. Feature selection and dimension reduction methods can also be used to reduce the number of dimensions (the number of unique taxa) to a more manageable number. One common approach is to aggregate unique taxa counts into higher taxonomic categories such as genus or family. (Ex: Aggregating a collection of 500 unique species to 73 unique genus). This notebook explores an alternative method for dimension reduction, word embedding, used in natural language processing.

Word embedding is a collection of natural language processing (NLP) methods that map a text word to a numeric vector for computation. An important feature of word embedding is that words with similar meaning (Ex: cheese and queso) have similar embeddings, irrespective of other text identifying traits such as spelling or country of origin. One method of creating word embeddings is to use co-occurrence data, or the count of which words co-occur with each other across multiple bodies of text (corpura [plural], corpus [single]). The reasoning for this method relies on the idea that words are more likely to co-occur in specific contexts. For example, when looking across a large collection of different documents or corpora, we might observe that words such as "cheese" and "queso" are likely to co-occur within the same documents, such as in a cookbook. These words have similiar meanings and therefore should have similiar embeddings. The information in the embeddings can then be used to predict what type of document a corpus might be based on its word content. Once we have embeddings for each unique word, we convert our corpus from a list of words to a list of embeddings, or document "properties".  The document properties are then used to identify what type of document it is. The presence of embeddings for "soup", "vegetable" and "knife" might suggest the corpus is a cookbook, while the embeddings for "steam", "lever", and "weight" would indicate the document is an engineering textbook. The similiarity of embeddings for words with similiar meaning means this classification is effective even if a particular cookbook prefers to use the word "queso" instead of "cheese" because "queso" and "cheese" have similiar embeddings. The labeling of documents is typically done with a machine learning classification algorithm and can be done with supervised (where each document is labeled beforehand) or unsupervised (where the computer clusters similiar documents for labeling later) methods.

Word embeddings can be used in microbiome analysis by recognizing the parallels betweem NLP text data and microbiome OTU data. One method of creating word embeddings is to use a Global Vectors for Word Representation Algorithm (GloVe) on a term-document matrix. A term-document matrix records the count of unique words across a group of samples. The term-document matrix is organized in a similiar manner to OTU tables, where words are equivalent to taxa and documents are equivalent to samples. Recognizing these parallels, we apply a word embedding algorithm to OTU data to learn taxa embeddings which capture some samentic property of the taxa. We then try use these embeddings to train a machine learning model to recognize which embeddings are indicative of sample properties.  Just as we can infer meaning from a document from the words it contains, here we try to determine host traits from a microbiome sample from its unique taxa.


### Workflow

This workflow uses a combination of "tidyverse" and "tidymodel" family packages for model training and analysis. These are a collection of packages designed to follow "tidy" data management principles and to work seemlessly together. Details of "tidy" package design can be read here (https://tidyverse.tidyverse.org/articles/manifesto.html)

### Environment

First we set our project environment. We are using a subset of the emp qiime mapping quality controlled filtered dataset. This soil data has already been subset and split into training, testing, and setaside datasets from in a previous file that must be run separately from this notebook.  
```{r}
dataFolder <- "data"
outputFolder <- "../../../../work/idoerg/hchung/outputs/soil"
sampleFilename <- "emp_qiime_mapping_qc_filtered.tsv"

#read sample data
sampledata <- read.csv(paste(dataFolder, sampleFilename, sep = "/"), sep = "\t") %>% 
  filter(grepl("soil", Description))

#read in RDS data
dataFolder <- "../../../../work/idoerg/hchung/"
otu_id <- readRDS(paste(dataFolder, "emp_deblur_90bp.qc_filtered_otu_taxonomy_list.RDS", sep = "/"))
soil_split <- readRDS(paste(dataFolder, "soil_split.RDS", sep = "/"))
soil_train <- training(soil_split)
soil_test <- testing(soil_split)
```
### Set model parameters

In this section we define some of our model parameters to be used later in the workflow
```{r}
#model Parameters
nrepeats <- 10 #define the number of folds to use during cross-validation
PAR = TRUE #We speed up our model runs on each fold by parallizing across multiple cores.
ncores <- 10 #Set the number of cores to use here

#tuning parameters
#set number of parameters to test for each model
rf_search_levels = 20
xgb_search_levels = 30

#iterations
#model variables to iterate over
resp <- "envo_biome_2" #response variable to predict
taxa_level = "phylum" #taxa level to aggregate OTUs to
modeltype = "randomforest" #model to make predictions (xgboost)
rank = 50 #rank/ no components for wmbedding space (any numeric value)
embedding = "base" #what embedding method to use (base, abundance, transformword, transformpca)

#set up parallel processing
if(PAR){
  cl <- makeForkCluster(ncores, outfile ="out.txt")
  registerDoParallel(cl)
  message(Sys.time()," REGISTERED ", ncores, " CORES")
}

```

### Dimension Reduction

To create word embeddings, we use the text2vec package's implementation of the GloVe algorithm. The GloVe algorithm collects word co-occurence statistics in the form of a word cooccurence matrix. While the text2vec package does provide functions for creating this matrix from a Corpus object, they are not easily used with our OTU data. However, it is relatively easy to create a co-occurence matrix by simply taking the crossproduct of our taxa-sample matrix and its transposed binary self. We write a custom function to create a co-occurence matrix from tidy data and include the option for symmetric or nonsymmetric co-occurence matrices. A symmetric co-occurence matrix counts the number of observations two variable co-occur and (as the name implies) is symmetric. A non-symmetric co-occurence matrix counts the number of times two variables co-occur together across all samples. The GloVe algorithm explicitly accepts the non-symmetric co-occurence matrix, but we include the option for symmetric co-occurence for later analysis. In our model workflow, we use these learned word embeddings to transform our OTU counts in our sample of interest to word embeddings, and then run the model on the transformed data. We also explore the option of learning word embeddings and transforming OTU counts with word embeddings learned from each new dataset rather than a cumulative reference.

We also compare word embeddings with PCA transformed data as a baseline for dimension reduction. We write a method to use PCA learned properties with R's builtin PCA functions.


```{r}
#function to create co-occurence matrix from a table (samples rows, terms columns)
cofxn <- function(x, sym){
  x <- data.matrix(x)
  x <- as(x, "dgTMatrix")
  y <- x > 0
    if(sym){
      z <- Matrix::crossprod(y)
    }else{
      z <- Matrix::crossprod(x,y)
    }
  return(z)
}

#function to create GloVe word embeddings
create_word_embedding <- function(input, sym = FALSE, rank = 50){
  cooccur_table <- cofxn((input), sym)
  glove = GlobalVectors$new(rank = rank, x_max = 1, learning_rate = 0.1) #hyperparameters
  wv_main = glove$fit_transform(cooccur_table, n_iter = 10, convergence_tol = 0.1, n_threads = 1)
  wv_context = glove$components
  word_vectors = wv_main + t(wv_context)
  return(word_vectors)
}

#function to create principal components
create_pca_embedding <- function(x_, rank = 50){
  x_ <- data.matrix(x_)
  pca_ <- prcomp(x_,rank = rank, center=TRUE)$rotation
  return(pca_)
}

otu_train <- select(soil_train, -contains("metadata")) #select OTU volumns from dataset 
pca_matrix <- create_pca_embedding(otu_train, rank = rank)
word_matrix <- create_word_embedding(otu_train, rank = rank)

```

### Aggregate data by taxa level

We want to compare word embeddings methods against other forms of dimension reduction such as OTU aggregation by taxa. Aggregating by taxa means collapsing individual OTU counts to summed counts over a higher taxonomic category such as genus or family. This also helps remove possible errors from OTU clustering where sequences slightly below or above the percent similarity threshold are inaccurately labeled. For species, we define separate cases for when OTUs in the dataset have distinct species annotations ("species") and when OTUs are known to be separate but are unannotated (otu). The "otu" option is equivalent to not aggregating by any of the taxonomy labels. 

```{r, warnings = FALSE}
custom_summarize_taxa <- function(input, taxa_level, otu_key){
  temp <- as.data.frame(t(input)) %>%
    rownames_to_column("otuid") %>%
    left_join(otu_key, by = "otuid") 

  taxa <- stri_remove_empty(str_remove(c("kingdom", "phylum", "class", "order", "family", "genus", "species", "otu"), taxa_level)) #remove all except chosen taxa

  suppressWarnings(
  res <- temp %>% #separate composite taxonomy information into separate columns
    separate(taxonomy, c("kingdom", "phylum", "class", "order", "family", "genus", "species"), extra = "drop", sep = "; ") %>%
    rename(otu = "otuid") %>% 
    select(-c(all_of(taxa))) %>%
    drop_na(!!taxa_level) 
  )

  #We save time on this step by using functions from "data.table", which is significantly faster than tidyverse tibbles.
  res <- data.table::setDT(res)[, lapply(.SD, as.numeric), by=taxa_level]
  res <- data.table::setDT(res)[, lapply(.SD, sum, na.rm = TRUE), by = taxa_level] #aggregate by unique taxa category

  remove <- paste(substr(taxa_level,1,1), "__", sep = "") #remove "$__" labels from OTU annotations
  
  res <- res %>%
    rename(taxonomy = all_of(taxa_level)) %>% #rename taxonomy column to selected taxa
    filter(taxonomy != remove) %>% #remove blank labels
    column_to_rownames("taxonomy") %>%
    t() %>% 
    as.data.frame() %>%
    janitor::clean_names()  

  return(res)
}
```

### Model Recipe

The recipes package ise used to create design matrices for modeling and to conduct preprocessing of variables. It is meant to be a more extensive framework that R's formula method. Some differences between simple formula methods and recipes are that

+ Variables can have arbitrary roles in the analysis beyond predictors and outcomes.

+ A recipe consists of one or more steps that define actions on the variables.

+ Recipes can be defined sequentially using pipes as well as being modifiable and extensible.

The recipes package follows "tidy" design principles and is designed to work with other tidyverse packages in a model workflow. 

```{r}
#select response variable to predict
response = paste("metadata", resp, sep = "__")

#define recipe
base_rec <- recipe(head(soil_train)) %>%
  update_role(contains("metadata"), new_role = "id") %>% #columns with metadata are id variables
  update_role(-contains("metadata"), new_role = "predictor") %>% #columns that don't contain metadata labels are predictors (OTU columns)
  update_role(matches(response), new_role = "outcome") %>% #response variable is defined as outcome variable
  step_naomit(all_outcomes(), skip = TRUE) %>% #remove NAs in response variable
  step_factor2string(all_outcomes(), skip = TRUE) %>% #converting strings to and from factors remove dropped factor levels
  step_string2factor(all_outcomes(), skip = TRUE) %>% 
  step_aggregate(all_predictors(), options = list("taxa_level" = taxa_level, "otu_key" = otu_id, names = TRUE)) #calls the custom summarize function

```

### Machine Learning

Here we use the tidymodels family of function for to tune and validate our model. Tidymodels follow a specific model running workflow that allows for quick deployment of models across multiple datasets (or subsets of datasets). A brief description involves defining the appropriate model, defining a number of "steps" to perform on each new dataset, and then running the steps and model (together called a "workflow") to multiple datasets (possibly in parallel). There are several models that can be used such as random forest, xgboost, convolutional neural networks, and support vector machines (for binary responses). In this workflow we will use a random forest model. 

#### Tuning

There are some model parameters (called hyperparameters) that we can adjust independent of our dataset. Depending on the data, adjusting the value of these hyperparameters can lead to improved model prediction performance. For some of these values, we can make informed decisions to pick the most optimal value. For example, random forest models perform reasonably well with 1000 trees, and previous studies have shown few improvements in changing the number of trees. For other values, such as mtry - the number of predictors that will be randomly sampled at each tree split, and min_n - the minimum number of datapoints in a node that are required for the node to be split further, the optimum value may depend on the model data. To choose the best combination of model parameters, we run the model across a range of values and then select which ones give the best performance. We can avoid overfitting the model by tuning our model with our training dataset using a k-fold cross validation scheme before validating the model on our test set.  

```{r}
switch(modeltype, 
       "randomforest" = {
         #model
         model <- rand_forest(
          trees = 1000,
          mtry = tune(),
          min_n = tune()
          ) %>% set_engine("ranger") %>% set_mode("classification")
         
         #search tune parameters
         search_grid <- grid_regular(
          mtry(range = c(10, 100)),
          min_n(range = c(1, 20)),
          levels = rf_search_levels)
         },
       "xgboost" = {
         #model 
         model <- boost_tree(
          trees = 1000, 
          tree_depth = tune(), 
          min_n = tune(), 
          loss_reduction = tune(),                     
          sample_size = tune(), 
          mtry = tune(),                               
          learn_rate = tune()                          
          ) %>% set_engine("xgboost") %>% set_mode("classification"); 
         #search tune parameters
          search_grid <- grid_latin_hypercube(
            tree_depth(),
            min_n(),
            loss_reduction(),
            sample_size = sample_prop(),
            finalize(mtry(), soil_train),
            learn_rate(),
            size = xgb_search_levels)
        }
      )
```
### Embedding Step

Tidymodels uses the term "recipe" to define a series of processing "steps" to use on the data. By predefining these steps independent of the data, we can quickly use the same model workflow across multiple independent datasets or multiple data subsets. In our workflow, we want to compare several different methods of transfsormed our variables (OTU counts) such as relative abundance, pca analysis, or word embedding. The tidymodels package's includes several pre-written steps for common data manipulations such as log transformation or to center data, but in our case we must define our own custom step functions (found in nltksoil_functions.R). Below is a description of the currently implemented steps.

+ **base** - no processing, predictors are OTU counts.

+ **abundance** - OTU counts are scaled to relative abundance within sample, values sum to 1 within a sample.

+ **transformword** - OTU counts are transformed into properties using word embeddings learned from the entire training set.

+ **transformpca** - OTU counts are transformed into properties using pca embeddings learned from the entire training set.

+ **word** - OTU counts are transformed using word embeddings learned from the current data.

+ **pca** - OTU counts are transformed using pca embeddings learned from the current data.

For word and PCA, current data can refer to independent datasets or folds of the data from a crossvalidation scheme.
```{r}

#define embedding space
switch(embedding, 
        "base"  = {rec <- base_rec},
        "abundance" = {rec <- step_abundance(base_rec, all_predictors())},
        "pca" = {rec <- step_cpca(base_rec, all_predictors(), options = list("rank" = rank, names = TRUE))},
        "word" = {rec <- step_word(base_rec, all_predictors(), options = list("rank" = rank, names = TRUE))}, 
        "transformword" = {rec <- step_transform(base_rec, all_predictors(), options = list("transform_embedding" = word_matrix, names = TRUE))},
        "transformpca" = {rec <- step_transform(base_rec, all_predictors(), options = list("transform_embedding" = pca_matrix, names = TRUE))}      
       )
```

### K-Cross validation and Running the Model

We use K-fold cross validation to split our training data into K number of folds. We then run the model K times where in each run, K-1 folds are used as the training data and 1 fold is used as the test set. We do to this measure the variance in our model and avoid bias model predictions from overfitting to a specific set of data. After tuning our model, we can look at the best performing models based on our preferred performance metric. We then use these parameters in our final model fit.

```{r, eval = FALSE}
#Define the tuning work flow
tune_workflow <- 
  workflow() %>% #workflow objects
  add_recipe(rec) %>% #add recipe
  add_model(model) #add model

#cross validation for training
#we stratify samples across the sample response to make sure that each fold has an even distribution of classes.
train_folds <- vfold_cv(soil_train, v = nrepeats, strata = response)

#check tune results
options(tidymodels.dark = TRUE)
#Define a vector of metric parameters to evaluate model accuracy
multi_metric <- metric_set(accuracy, bal_accuracy, f_meas, kap, npv, ppv, precision, recall, sens, pr_auc, roc_auc)
message("TUNING MODEL")
tune_results <- tune_grid(tune_workflow,  #tune a parameter grid using the tune_workflow
                          resamples = train_folds,  #apply the workflow to folds created by vfold_cv
                          grid = search_grid,  #search of provided list of parameters
                          #metrics = multi_metric, #calculate performance using metrics in metric set
                          control = control_grid( #control parameters to tuning process
                            verbose = TRUE, #print progress of tuning to console
                            allow_par = TRUE, #allow parallel processing if parallel backend registers
                            #extract = function (x) {extract_model(x)}, #extract model specificiations for each tune iterations
                            save_pred = TRUE)) #save model predictions 

#what are the best parameters
best <- show_best(tune_results, metric = "accuracy", n = 5) #pick top n results from tuning based on metric
best_parameters <- select_best(tune_results, metric = "accuracy")

final_workflow <- tune_workflow %>%
  finalize_workflow(best_parameters) 

final_model <- parsnip::fit(final_workflow, training(soil_split))
```

### Fitting final model

From the results of our tuning, we can select the best hyperparameters to make our model. We then fit our chosen model to our training data before validating it on our testing set. We bootstrap our testing set to avoid overfitting and to get a range of results to generate confidence intervals. Bootstrapping is a resampling method that generates subsets of data using random sampling with replacement.Lastly, we want to compare the results of our model predictions to a custom null model which predicts classes based on the proportion of samples in the data. 

```{r, eval = FALSE}
#create custom model prediction function based off proportion of classes within data (equivalent to guessing)
custom_nullsample <- function(x, response = "blank"){ #take in a splits object which defines the data splits created by vfold_cv
  train <- analysis(x)[[response]] #take out class categories from training data
  test <- assessment(x)[[response]] #take out class categories from testing data
  preds = sample(size = length(test), x = names(table(train)), prob = as.numeric(table(train)), replace = T) #make n draws, where n is the length of responses in the test set, from the classes in the training set with equal probability to their proportions in the testing set with replacement
  res <- tibble(estimate = preds, response = test) #merge test responses with test predictions
  return(res) #return results
}

tune_results <- tune_results %>% 
  mutate(.null = future_map(splits, custom_nullsample, response = response)) %>% 
  mutate(.predictions = future_map(.predictions, ~rename_(., response = response))) %>%#create null guess
  select(-c("splits")) #remove splits object which takes a majority of the data within the tune_results objects

soil_bootstraps <- bootstraps(soil_test,times = nrepeats, strata = response) #Create bootstrapped datasets of testing data

test_results <- soil_bootstraps %>%
  mutate(.probabilities = future_map(splits, ~predict(final_model, new_data =  analysis(.), type = "prob"))) %>% #probability predictions using final model on bootstrapped data splits
  mutate(.predictions = future_map(splits, ~predict(final_model, analysis(.)))) %>% #class predictions
  mutate(.predictions = future_map2(splits, .predictions, ~cbind(analysis(.x)[[response]], .y))) %>% #bind predictions with truth
  mutate(.predictions = future_map(.predictions, ~rename(., response = "analysis(.x)[[response]]"))) %>% #rename table to predictions
  mutate(.predictions = future_map2(.predictions, .probabilities, cbind)) %>% #bind predictions/truth with probability estimates
  mutate(.predictions = future_map(.predictions, ~mutate(., response = droplevels(response)))) %>% #remove extra factor levels
  select(-c(.probabilities)) #remove probabilities

#for null model estimates, pull the truth responses from training set
train = soil_train[[response]]

#a second null sample function with the option to provide training proportions
custom_test_null <- function(x, y, response){
  test <- analysis(x)[[response]]
  preds <- sample(size = length(test), x = names(table(y)), prob = as.numeric(table(y)), replace = T)
  res <- tibble(estimate = preds, response = test)
}

test_results <- test_results %>% 
  mutate(.null = future_map(splits, custom_test_null, y = train, response = response)) %>% #create null guess using proportions of truth response from training set
  select(-c("splits")) 


final_results <- list(tune_results, best, test_results, final_model)
names(final_results) <- c("tune", "best", "test", "model")
#save final results as object along with model parameters as name.
saveRDS(final_results, paste(outputFolder,"/", taxa_level,"_", resp,"_", modeltype, "_", rank, "_", embedding, ".RDS", sep = ""))

```

